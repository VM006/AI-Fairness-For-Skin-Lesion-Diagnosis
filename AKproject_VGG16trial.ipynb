{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4qqO9QD4oVU"
      },
      "outputs": [],
      "source": [
        "# Program Outline\n",
        "# ===============\n",
        "\n",
        "# 1. Read CSV file of only benign and malignant images\n",
        "# 2. Train-test split\n",
        "# 3. Instantiate VGG16 model\n",
        "# 4. Prepare for model training\n",
        "# 5. Train and save model, printing metrics/results\n",
        "# 6. Test model\n",
        "# 7. Print results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all relevant packages here\n",
        "# =================================\n",
        "\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import threading\n",
        "import itertools\n",
        "import sys\n",
        "import time"
      ],
      "metadata": {
        "id": "_SWdreUfrUer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standalone functions\n",
        "# ====================\n",
        "\n",
        "def load_images(path, names):\n",
        "  '''Load all images from fitz17k denoted by a list of image names'''\n",
        "  image_list = []\n",
        "  for i in range (0,len(X_names)):\n",
        "    img_file = os.path.join(path, X_names[i]+'.jpg')\n",
        "    image = np.array(Image.open(img_file))\n",
        "    # Normalise image into range 0-1\n",
        "    if (np.max(image)-np.min(image) != 0):\n",
        "      image = (image-np.min(image))/(np.max(image)-np.min(image))\n",
        "\n",
        "    # Append to list of images\n",
        "    image_list.append(image)\n",
        "\n",
        "  # Convert image list to np array, and return\n",
        "  image_array = np.asarray(image_list)\n",
        "\n",
        "  return image_array"
      ],
      "metadata": {
        "id": "UfkkV-JLrhFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate user\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "lITVU1TMD3HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unmount first if previously mounted\n",
        "try:\n",
        "    drive.flush_and_unmount()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Mount Google drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "dp = '/content/drive/My Drive/Colab Notebooks/AKproject'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qaa8nr1YGWZe",
        "outputId": "f12318b6-3f1a-48bb-9720-a08d5f682505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Read CSV file of only benign and malignant images\n",
        "# ====================================================\n",
        "\n",
        "csv_name = os.path.join(dp, 'fitzpatrick17k_B&Monly.csv')\n",
        "df = pd.read_csv(csv_name)\n",
        "df.head()\n",
        "data = df.to_numpy()\n",
        "X = data[:,0]\n",
        "y_string = data[:,5]\n",
        "print(X[0:5])\n",
        "print(y_string[0:5])\n",
        "\n",
        "# Convert strings in y to numbered labels\n",
        "y = preprocessing.LabelEncoder().fit_transform(y_string)\n",
        "print(y[0:5])\n",
        "\n",
        "# Convert y to tensor\n",
        "y = torch.from_numpy(y)\n",
        "\n",
        "# Print sizes\n",
        "print(f\"Size of X: {X.shape[0]}\")\n",
        "print(f\"Size of y: {y.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9jYbLD4jhUO",
        "outputId": "3e4cc5e5-408f-4d3c-af65-b0e745784ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['d2bac3c9e4499032ca8e9b07c7d3bc40' '45f7fe0e10214e32e890cad9d29d4811'\n",
            " 'b87804452f60aa162a6d29c0f66a2466' '4c3f795cf8eb72b946f9bd2642cf23c1'\n",
            " '99247c9fe486aa9ab71686c8e676c135']\n",
            "['benign' 'malignant' 'malignant' 'malignant' 'benign']\n",
            "[0 1 1 1 0]\n",
            "Size of X: 4497\n",
            "Size of y: 4497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Train-test split\n",
        "# ===================\n",
        "\n",
        "# Stratify train-test split by labels, 25% test set size\n",
        "stratify = y\n",
        "test_size = 0.25\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=stratify, test_size=test_size)\n",
        "\n",
        "print(f\"Size of X_train: {X_train.shape[0]}\")\n",
        "print(f\"Size of X_test: {X_test.shape[0]}\")\n",
        "print(f\"Size of y_train: {y_train.shape[0]}\")\n",
        "print(f\"Size of y_test: {y_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "Z2bwvv2RsJgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "967ebdbe-eeee-4df2-9001-edb332ff1ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of X_train: 3372\n",
            "Size of X_test: 1125\n",
            "Size of y_train: 3372\n",
            "Size of y_test: 1125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiating a Model + Image Transforms\n",
        "We're going to follow  the architecture outlined in the Fitzpatrick 17k's debut paper: https://arxiv.org/pdf/2104.09957\n",
        "\n",
        "\n",
        "The model used in the paper is VGG16 with all gradients frozen bar the final dense layer, which was modified from\n",
        "\n",
        "- nn.Linear(4096, 1000, bias=True)\n",
        "\n",
        "to\n",
        "\n",
        "- nn.Sequential(nn.Linear(4096, 256), nn.ReLU(), nn.Dropout(0.4), nn.Linear(256, len(label_codes)), nn.LogSoftmax(dim=1))\n",
        "\n",
        "They also make use of the Adam optimizer and Negative Log-Likelihood loss function, which does require explicit denotion of the Softmax activation function in the final dense layer.\n",
        "\n",
        "As for the image transforms for the purpose of data augmentation:\n",
        "- Conversion to Python Imaging Library image object (compatibility)\n",
        "- Random crop over [80%,100%] of the image area, before resizing to 256×256\n",
        "- Random rotation up to ±15°\n",
        "- Random brightness, contrast, saturation and hue shift\n",
        "- Random left-to-right image flip, 50% chance\n",
        "- Crop the centre 224×224 region from the now 256×256 image to match ImageNet standards. This is because VGG16 architecture by default assumes images are of this dimension\n",
        "- Converts the now PIL image to a PyTorch tensor, giving it shape:[Colours,Height,Width] and normalised pixel values in [0,1], since any PyTorch model requires a tensor as input\n",
        "- Normalisation of the image by the top row of means and bottom row of standard deviations in accordance with ImageNet means and SDs. Not that this cannot be used on PIL objects but tensors only, and is done per channel i.e. (pseudocode)\n",
        "\n",
        "  NormalisationTensor = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n",
        "  meanNorm = NormalisationTensor[0]  # [0.485, 0.456, 0.406]\n",
        "  stdNorm = NormalisationTensor[1]   # [0.229, 0.224, 0.225]\n",
        "\n",
        "  For input tensor t and each colour channel n\n",
        "      out[n] = (t[n] - meanNorm[n]) / stdNorm[n]"
      ],
      "metadata": {
        "id": "qiU630g5auq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Instantiate VGG16 model\n",
        "# ==========================\n",
        "\n",
        "model = models.vgg16(pretrained=True, progress=True)\n",
        "# Pretraining makes training faster as the model isn't completely naive\n",
        "# Progress bar to show download progress\n",
        "# Model download only has to be done once per runtime\n",
        "\n",
        "# We're going to follow NN architecture identical to that from the Fitzpatrick\n",
        "# 17k paper, but modify it for binary classification\n",
        "print(f\"Number of label types: {len(y.unique())}\") # Sanity check\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False # Freeze all gradients in the pretrained VGG16 model\n",
        "model.classifier[6] = nn.Sequential(nn.Linear(4096, 256),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(0.4),\n",
        "                                    nn.Linear(256, len(y.unique())),\n",
        "                                    nn.Sigmoid()) # Use sigmoid for binary classification\n",
        "\n",
        "# Print number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "print(f\"Number of trainable parameters: {total_trainable_params}\")\n",
        "print(f\"Percentage of trainable parameters: {np.round(total_trainable_params*100/total_params, 2)}%\")\n",
        "\n",
        "# Instantiate loss function and optimizer\n",
        "loss_fun = nn.NLLLoss()\n",
        "optim = torch.optim.Adam(model.classifier[6].parameters())\n",
        "\n",
        "# Check if CUDA is available, set device, empty cache, move model and loss function to device\n",
        "CUDA = torch.cuda.is_available()\n",
        "print(f\"CUDA Status: {CUDA}\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if CUDA:\n",
        "  torch.cuda.empty_cache()\n",
        "model.to(device)\n",
        "loss_fun = loss_fun.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQtpvRe2YKPF",
        "outputId": "5aa21b97-6b44-4f31-d0b6-a04a4b855153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of label types: 2\n",
            "Total number of parameters: 135309890\n",
            "Number of trainable parameters: 1049346\n",
            "Percentage of trainable parameters: 0.78%\n",
            "CUDA Status: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By modifying the final dense layer, we also \"unfreeze\" it - so during\n",
        "training, only the final layer of the classifier network is trained. This is for a few reasons:\n",
        "- Faster training - fewer parameters to update during backpropagation\n",
        "- Transfer learning - the pretrained VGG16 learned good feature extractors from ImageNet already, so keeping these weights is preferred and helps keep the model from overfitting to the training data"
      ],
      "metadata": {
        "id": "zWKXPPiIgZ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Prepare for Model Training\n",
        "# ===============================\n",
        "\n",
        "# Create list of image transforms for data augmentation\n",
        "# We will take this straight from the Fitzpatrick 17k paper\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.CenterCrop(size=224),  # Image net standards\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Set number of epochs, batch size\n",
        "n_epochs = 1\n",
        "batch_size = 256\n",
        "# There are 3372 training samples, so 14 epochs required for a full pass with a batch size of 256"
      ],
      "metadata": {
        "id": "rBYPCqAFZ-nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wUr-BWB5J3_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}